<!DOCTYPE html>
<html>
<head>
    <title>Voice Assistant Interface</title>
    <style>
        body {
            font-family: Arial, sans-serif;
            max-width: 800px;
            margin: 0 auto;
            padding: 20px;
        }
        #transcript-container {
            margin: 20px 0;
            padding: 10px;
            border: 1px solid #ccc;
            min-height: 100px;
        }
        .status-active {
            color: green;
        }
        .status-inactive {
            color: red;
        }
        button {
            padding: 10px 20px;
            margin: 5px;
            font-size: 16px;
        }
    </style>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/socket.io/4.0.1/socket.io.js"></script>
</head>
<body>
    <h1>Voice Assistant Interface</h1>
    <button onclick="startListening()" id="startBtn">Start Listening</button>
    <button onclick="stopListening()" id="stopBtn">Stop Listening</button>
    <div id="status">
        <p>Status: <span id="listening-status" class="status-inactive">Not Listening</span></p>
    </div>
    <div id="transcript-container">
        <h3>Real-time Transcript:</h3>
        <p id="interim"></p>
        <p id="final"></p>
    </div>
    <div id="response-container">
        <h3>Assistant Response:</h3>
        <p id="response">None</p>
    </div>

    <script>
        const socket = io();
        let recognition;
        let isListening = false;

        // Initialize speech recognition
        function initializeSpeechRecognition() {
            recognition = new (window.SpeechRecognition || window.webkitSpeechRecognition)();
            recognition.interimResults = true;
            recognition.continuous = true;
            recognition.lang = 'en-US';

            recognition.onresult = (event) => {
                let interimTranscript = '';
                let finalTranscript = '';

                for (let i = event.resultIndex; i < event.results.length; i++) {
                    const transcript = event.results[i][0].transcript;
                    if (event.results[i].isFinal) {
                        finalTranscript += transcript;
                        socket.emit('transcript', { transcript: transcript });
                    } else {
                        interimTranscript += transcript;
                    }
                }

                document.getElementById('interim').textContent = interimTranscript;
                if (finalTranscript) {
                    document.getElementById('final').textContent = finalTranscript;
                }
            };

            recognition.onerror = (event) => {
                console.error('Speech recognition error:', event.error);
                stopListening();
            };
        }

        function startListening() {
            if (!recognition) {
                initializeSpeechRecognition();
            }
            recognition.start();
            isListening = true;
            socket.emit('start_listening');
            updateStatus(true);
        }

        function stopListening() {
            if (recognition) {
                recognition.stop();
            }
            isListening = false;
            socket.emit('stop_listening');
            updateStatus(false);
        }

        function updateStatus(listening) {
            const statusElement = document.getElementById('listening-status');
            statusElement.textContent = listening ? 'Listening' : 'Not Listening';
            statusElement.className = listening ? 'status-active' : 'status-inactive';
        }

        // Socket.io event handlers
        socket.on('response', (data) => {
            document.getElementById('response').textContent = data.response;
        });

        socket.on('status_update', (data) => {
            updateStatus(data.listening);
        });

        // Handle page unload
        window.onbeforeunload = function() {
            if (recognition) {
                recognition.stop();
            }
        };
    </script>
</body>
</html>
